{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6e8cf7",
   "metadata": {},
   "source": [
    "### üìÅ Available LoRa Models\n",
    "\n",
    "| Model Size | Motion Type | LoRa Name | Description |\n",
    "|-------|-------------|-----------|-------------|\n",
    "| Lite, Pro | Object Rotation | `Microwave-right` | Rotates object ~360¬∞ to the right over 5 seconds |\n",
    "| Lite, Pro | Object Rotation | `Microwave-left` | Rotates object ~360¬∞ to the left over 5 seconds |\n",
    "| Lite, Pro | Camera Movement | `Arc-right` | Camera arcs ~360¬∞ around object to the right over 5 seconds |\n",
    "| Lite, Pro | Camera Movement | `Arc-left` | Camera arcs ~360¬∞ around object to the left over 5 seconds |\n",
    "| Lite | Camera Movement | `Dolly-in` | Camera moves forward toward subject |\n",
    "| Lite | Camera Movement | `Dolly-out` | Camera moves backward from subject |\n",
    "| Lite | Camera Movement | `Truck-right` | Camera tracks horizontally to the right |\n",
    "| Lite | Camera Movement | `Truck-left` | Camera tracks horizontally to the left |\n",
    "\n",
    "<br>\n",
    "\n",
    ">  Note:\n",
    "> 1. For optimal performance, select an appropriate LoRa for the **t2v** (text-to-video) or **i2v** (image-to-video) modality.  \n",
    ">    We named our LoRas using the template: `kandinskylab/Kandinsky-5.0-{Modality}-{Model_Size}-LoRa-{LoRa_Name}` where `Modality` is either `I2V` or `T2V`.\n",
    "> 2. Additional camera movements in the **Pro** model can be achieved directly via the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135f0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import torch\n",
    "from kandinsky import get_I2V_pipeline\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99a497",
   "metadata": {},
   "source": [
    "### Load pipe \n",
    "first, ensure that you have downloaded the models using `download_models.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5198a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e79994ec9d4bc0b07ebd7afff48e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Magcache\n"
     ]
    }
   ],
   "source": [
    "pipe = get_I2V_pipeline(\n",
    "    device_map={\"dit\": \"cuda:0\", \"vae\": \"cuda:0\", \"text_embedder\": \"cuda:0\"},\n",
    "\tconf_path=\"./configs/k5_lite_i2v_5s_sft_sd.yaml\",\n",
    "    magcache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ce030",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Download LoRa adapter from HuggingFaceü§ó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19e34de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5690b76efa4fdbbaa6f7064d31b703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02964cc731648889004c1eb5ba7e4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b98e408af04d2baee623b0e951d6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af71ecdad7c5415c93f7d92392b14fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "examples/girl.mp4:   0%|          | 0.00/4.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9efe667b7f4915b269c04496ac7b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "examples/cheburashka.mp4:   0%|          | 0.00/1.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cache_dir = './weights'\n",
    "lora_name = 'Microwave-right'\n",
    "repo_id = f\"kandinskylab/Kandinsky-5.0-I2V-Lite-LoRa-{lora_name}\"\n",
    "adapter_path = snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    local_dir=os.path.join(cache_dir, repo_id),\n",
    "    token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e988366",
   "metadata": {},
   "source": [
    "Load the adapter into the pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0dd4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.load_adapter(\n",
    "    adapter_config=os.path.join(cache_dir, repo_id, \"config_lora.json\"),\n",
    "    adapter_path= os.path.join(cache_dir, repo_id, \"lora.safetensors\"),\n",
    "    adapter_name=lora_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6fce3",
   "metadata": {},
   "source": [
    "> Note: <br>\n",
    "> The trigger words we use are stored in the LoRA safetensors metadata. They are automatically concatenated with prompts during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80df1e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Microwave-right': 'M1CR0W4V3 r0t4tION, 360-degree rotation. '}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.peft_triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1b081",
   "metadata": {},
   "source": [
    "Generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e2a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:26<00:00,  2.92s/it]\n",
      "/home/user/conda/envs/kandinsky-cuda12.8/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_dir = './outputs/loras/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "out = pipe(\n",
    "    \"A bear wearing an ushanka hat and holding a balalaika in a snowy forest is dancing.\",\n",
    "    image = \"assets/test_image.jpg\",\n",
    "    time_length=5,\n",
    "    num_steps=50,\n",
    "    save_path=os.path.join(save_dir, f'i2v_lite_{lora_name}.mp4'),\n",
    "    seed=12345,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cecc728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./outputs/loras/i2v_lite_Microwave-right.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(os.path.join(save_dir, f'i2v_lite_{lora_name}.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50dc504",
   "metadata": {},
   "source": [
    "Download another adapter and switch to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e074490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e855ab13bb44663ab0e0f57873bb69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_name = 'Microwave-left'\n",
    "repo_id = f\"kandinskylab/Kandinsky-5.0-I2V-Lite-LoRa-{lora_name}\"\n",
    "adapter_path = snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    local_dir=os.path.join(cache_dir, repo_id),\n",
    "    token=None\n",
    ")\n",
    "\n",
    "pipe.load_adapter(\n",
    "    adapter_config=os.path.join(cache_dir, repo_id, \"config_lora.json\"),\n",
    "    adapter_path= os.path.join(cache_dir, repo_id, \"lora.safetensors\"),\n",
    "    adapter_name=lora_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:41<00:00,  2.02s/it]\n",
      "/home/user/conda/envs/kandinsky-cuda12.8/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./outputs/loras/i2v_lite_Microwave-left.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pipe(\n",
    "    \"A bear wearing an ushanka hat and holding a balalaika in a snowy forest is dancing.\",\n",
    "    image=\"assets/test_image.jpg\",\n",
    "    time_length=5,\n",
    "    num_steps=50,\n",
    "    save_path=os.path.join(save_dir, f'i2v_lite_{lora_name}.mp4'),\n",
    "    seed=12345,\n",
    ")\n",
    "Video(os.path.join(save_dir, f'i2v_lite_{lora_name}.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19752f7d",
   "metadata": {},
   "source": [
    "Switch to the previous loaded LoRa adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7116db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_lora_name = 'Microwave-right'\n",
    "pipe.set_adapter(prev_lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b373622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:41<00:00,  2.02s/it]\n",
      "/home/user/conda/envs/kandinsky-cuda12.8/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./outputs/loras/i2v_lite_Microwave-right_v2.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pipe(\n",
    "    \"A bear wearing an ushanka hat and holding a balalaika in a snowy forest is dancing.\",\n",
    "    image=\"assets/test_image.jpg\",\n",
    "    time_length=5,\n",
    "    num_steps=50,\n",
    "    save_path=os.path.join(save_dir, f'i2v_lite_{prev_lora_name}_v2.mp4'),\n",
    "    seed=12345,\n",
    ")\n",
    "Video(os.path.join(save_dir, f'i2v_lite_{prev_lora_name}_v2.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8538c3",
   "metadata": {},
   "source": [
    "Disable all adapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb9f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:37<00:00,  1.95s/it]\n",
      "/home/user/conda/envs/kandinsky-cuda12.8/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./outputs/loras/i2v_lite_default.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pipe(\n",
    "    \"A bear wearing an ushanka hat and holding a balalaika in a snowy forest is dancing.\",\n",
    "    image=\"assets/test_image.jpg\",\n",
    "    time_length=5,\n",
    "    num_steps=50,\n",
    "    save_path=os.path.join(save_dir, f'i2v_lite_default.mp4'),\n",
    "    seed=12345,\n",
    ")\n",
    "Video(os.path.join(save_dir, f'i2v_lite_default.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df629af",
   "metadata": {},
   "source": [
    "Set already loaded adapter again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee69802",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_adapter(prev_lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df2178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./outputs/loras/i2v_lite_Microwave-right_v3.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pipe(\n",
    "    \"A bear wearing an ushanka hat and holding a balalaika in a snowy forest is dancing.\",\n",
    "    image=\"assets/test_image.jpg\",\n",
    "    time_length=5,\n",
    "    num_steps=50,\n",
    "    save_path=os.path.join(save_dir, f'i2v_lite_{prev_lora_name}_v3.mp4'),\n",
    "    seed=12345,\n",
    ")\n",
    "Video(os.path.join(save_dir, f'i2v_lite_{prev_lora_name}_v3.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53761aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kandinsky-cuda12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
